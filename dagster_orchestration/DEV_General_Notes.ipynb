{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Notes:\n",
    "\n",
    "INPUT: \"year-month\"\n",
    "\n",
    "OUTPUT: Pandas DataFrame\n",
    "\n",
    "\n",
    "Should:\n",
    "Schedule the full list\n",
    "\n",
    "\n",
    "Industrialize it\n",
    "Host it\n",
    "Deploy it to production\n",
    "Make it recurring\n",
    "Monitor the runs\n",
    "\n",
    "\n",
    "Other:\n",
    "one directory per year, and one subdirectory per month\n",
    "\n",
    "always named \"events.csv\"\n",
    "\n",
    "\n",
    "My take:\n",
    "https://docs.dagster.io/concepts/io-management/io-managers#when-to-use-io-managers\n",
    "\n",
    "This gives us the possibility to dive dep into Pydantic:\n",
    "https://docs.dagster.io/concepts/io-management/io-managers#defining-pythonic-io-managers\n",
    "\n",
    "\n",
    "https://docs.dagster.io/_apidocs/libraries/dagster-aws#dagster_aws.s3.S3Coordinate\n",
    "\n",
    "\n",
    "So, while process_s3_files_job() might seem unnecessary because it's just calling create_asset_from_s3_file(), it's actually doing a lot of important work behind the scenes by integrating with Dagster's job management, configuration, and resource management features.\n",
    "\n",
    "\n",
    "\n",
    "Ã  ameliorer:\n",
    "\n",
    "2. **S3 Bucket Name**: In `dagster_orchestration/sensors/sensors.py`, the bucket name is hardcoded as \"xxx-xxx-xxx\". If this bucket name changes or if you want to use different buckets for different environments (like testing, staging, production), consider making this a configurable parameter or an environment variable.\n",
    "\n",
    "3. **Error Handling**: There doesn't seem to be much error handling in the code. For example, what happens if `s3_client.get_object(Bucket=bucket, Key=key)` fails because the file doesn't exist or because of a network error? Adding some error handling could make your code more robust.\n",
    "\n",
    "\n",
    "for arguments:\n",
    "\n",
    "4. **File Structure**: Your file structure is clear and well-organized. Each Python file has a specific purpose (defining assets, sensors, schedules, etc.), which makes your code easier to understand and maintain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class S3WithBucketResource(ConfigurableResource):\n",
    "    bucket_name: str = Field(String, description=\"S3 bucket name\")\n",
    "\n",
    "    def create_resource(self, context):\n",
    "        s3_resource = S3Resource(context)\n",
    "        s3_resource.bucket_name = self.bucket_name\n",
    "        return s3_resource\n",
    "\n",
    "\n",
    "  # OR THIS\n",
    "\n",
    "class S3WithBucketResource(S3Resource):\n",
    "    bucket_name: str = Field(String, description=\"S3 bucket name\")\n",
    "\n",
    "    def __init__(self, context):\n",
    "        super().__init__(context)\n",
    "        self.bucket_name = context.resource_config[\"bucket_name\"]\n",
    "\n",
    "    def get_bucket_name(self):\n",
    "        return self.bucket_name\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this\n",
    "#TB! load env ariables\n",
    "import our_utilities; our_utilities.load_env_with_substitutions()\n",
    "\n",
    "from dagster import sensor, RunRequest\n",
    "from dagster.core.instance import DagsterInstance\n",
    "from ..jobs.jobs import process_s3_files_job\n",
    "\n",
    "@sensor(job=process_s3_files_job)\n",
    "def check_for_new_s3_files(context):\n",
    "    s3_client = context.resources.s3\n",
    "    bucket = context.resources.s3_with_bucket.bucket_name\n",
    "    dagster_instance = DagsterInstance.get()\n",
    "    paginator = s3_client.get_paginator('list_objects')\n",
    "    for result in paginator.paginate(Bucket=bucket):\n",
    "        for file in result.get('Contents', []):\n",
    "            file_name = file['Key']\n",
    "            if file_name.endswith('events.csv'):\n",
    "                year, month, _ = file_name.split('/')\n",
    "                asset_key = f\"{year}_{month}_events\"\n",
    "                if not dagster_instance.has_asset_key(asset_key):\n",
    "                    yield RunRequest(run_key=file_name, run_config={\n",
    "                        \"ops\": {\n",
    "                            \"create_asset_from_s3_file\": {\n",
    "                                \"inputs\": {\n",
    "                                    \"s3_coordinate\": {\n",
    "                                        \"bucket\": bucket,\n",
    "                                        \"key\": file_name\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "\n",
    "# In this code:\n",
    "\n",
    "# dagster_instance = instance.DagsterInstance.get() gets the current Dagster instance.\n",
    "# if not dagster_instance.has_asset_key(asset_key): checks if the asset catalog of the Dagster instance already has an asset with the given key. If not, it yields a RunRequest for the process_s3_files_job job.\n",
    "# This way, the sensor will only trigger the job for files that have not yet been processed into assets.\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "#or this\n",
    "     from dagster import sensor, RunRequest\n",
    "from dagster.core.instance import DagsterInstance\n",
    "from dagster_aws.s3 import S3Coordinate\n",
    "\n",
    "@sensor\n",
    "def check_for_new_s3_files(context):\n",
    "    s3_client = context.resources.s3\n",
    "    bucket = context.resources.s3_with_bucket.bucket_name\n",
    "    dagster_instance = DagsterInstance.get()\n",
    "    paginator = s3_client.get_paginator('list_objects')\n",
    "    for result in paginator.paginate(Bucket=bucket):\n",
    "        for file in result.get('Contents', []):\n",
    "            file_name = file['Key']\n",
    "            if file_name.endswith('events.csv'):\n",
    "                year, month, _ = file_name.split('/')\n",
    "                asset_key = f\"{year}_{month}_events\"\n",
    "                if not dagster_instance.has_asset_key(asset_key):\n",
    "                    yield RunRequest(run_key=file_name, run_config={\n",
    "                        \"ops\": {\n",
    "                            \"create_asset_from_s3_file\": {\n",
    "                                \"inputs\": {\n",
    "                                    \"s3_coordinate\": {\n",
    "                                        \"bucket\": bucket,\n",
    "                                        \"key\": file_name\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    })               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dagster import ConfigurableResource\n",
    "from dagster_aws.s3.resources import S3Resource\n",
    "\n",
    "class S3WithBucketResource(ConfigurableResource):\n",
    "    bucket_name: str\n",
    "\n",
    "    def create_resource(self, context):\n",
    "        s3_resource = S3Resource(context)\n",
    "        s3_resource.bucket_name = self.bucket_name\n",
    "        return s3_resource\n",
    "\n",
    "    def get_bucket_name(self):\n",
    "        return self.bucket_name\n",
    "    \n",
    "\n",
    "\n",
    "# or this\n",
    "\n",
    "from dagster import ConfigurableResource, InitResourceContext\n",
    "from dagster_aws.s3.resources import S3Resource\n",
    "\n",
    "class S3WithBucketResource(S3Resource):\n",
    "    def __init__(self, context: InitResourceContext, bucket_name: str):\n",
    "        super().__init__(context)\n",
    "        self.bucket_name = bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dagster import ConfigurableResource\n",
    "from dagster_aws.s3.resources import S3Resource\n",
    "\n",
    "class S3WithBucketResource(ConfigurableResource):\n",
    "    bucket_name: str\n",
    "\n",
    "    def create_resource(self, context):\n",
    "        s3_resource = S3Resource(context)\n",
    "        s3_resource.bucket_name = self.bucket_name\n",
    "        return s3_resource\n",
    "\n",
    "    def get_bucket_name(self):\n",
    "        return self.bucket_name\n",
    "    \n",
    "\n",
    "\n",
    "# or this\n",
    "    \n",
    "from dagster import ConfigurableResource, InitResourceContext\n",
    "from dagster_aws.s3.resources import S3Resource\n",
    "\n",
    "class S3WithBucketResource(ConfigurableResource):\n",
    "    bucket_name: str\n",
    "\n",
    "    def __init__(self, bucket_name: str):\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "    def create_resource(self, context: InitResourceContext):\n",
    "        s3_resource = S3Resource(context)\n",
    "        s3_resource.bucket_name = self.bucket_name\n",
    "        return s3_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TB! load env ariables\n",
    "import our_utilities; our_utilities.load_env_with_substitutions()\n",
    "\n",
    "from dagster import ConfigurableResource, InitResourceContext\n",
    "from dagster_aws.s3.resources import S3Resource\n",
    "\n",
    "class S3WithBucketResource(ConfigurableResource):\n",
    "    bucket_name: str\n",
    "\n",
    "    def __init__(self, bucket_name: str):\n",
    "        super().__init__()\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "    def create_resource(self, context: InitResourceContext):\n",
    "        s3_resource = S3Resource(context)\n",
    "        s3_resource.bucket_name = self.bucket_name\n",
    "        return s3_resource\n",
    "    \n",
    "\n",
    "\n",
    "# or this\n",
    "\n",
    "\n",
    "from dagster import resource\n",
    "from dagster_aws.s3.resources import s3_resource\n",
    "\n",
    "@resource(config_schema={\"bucket_name\": str})\n",
    "def s3_with_bucket_resource(context):\n",
    "    bucket_name = context.resource_config[\"bucket_name\"]\n",
    "    s3 = s3_resource(context)\n",
    "    s3.bucket_name = bucket_name\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

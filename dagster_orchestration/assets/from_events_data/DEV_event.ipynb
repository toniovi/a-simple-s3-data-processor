{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TB! load env ariables\n",
    "import our_utilities; our_utilities.load_env_with_substitutions()\n",
    "\n",
    "from dagster import asset, AssetIn, AssetKey, AssetMaterialization, AssetExecutionContext, Output, DynamicOutput, DynamicOut\n",
    "from dagster.core.instance import DagsterInstance\n",
    "\n",
    "# Import DataFrame from pandas\n",
    "from pandas import DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ...resources.resources import MyAWSS3Resource\n",
    "\n",
    "@asset\n",
    "def get_new_s3_files(context: AssetExecutionContext,\n",
    "                     s3_with_bucket: MyAWSS3Resource):\n",
    "    s3_client = s3_with_bucket.create_s3_client()\n",
    "    bucket = s3_with_bucket.bucket_name\n",
    "\n",
    "    dagster_instance = DagsterInstance.get()\n",
    "\n",
    "    paginator = s3_client.get_paginator('list_objects')\n",
    "    new_files = []\n",
    "    for result in paginator.paginate(Bucket=bucket):\n",
    "        for file in result.get('Contents', []):\n",
    "            file_name = file['Key']\n",
    "            if not dagster_instance.has_asset_key(AssetKey(file_name)):\n",
    "                new_files.append({'bucket': bucket, 'key': file_name})\n",
    "    return new_files\n",
    "\n",
    "\n",
    "@asset(out=DynamicOut(dagster_type=DataFrame),\n",
    "       ins={\"new_files\": AssetIn(key=AssetKey(\"get_new_s3_files\"))})\n",
    "def create_assets_from_s3_files(context: AssetExecutionContext, \n",
    "                                s3_with_bucket: MyAWSS3Resource,\n",
    "                                new_files: list):\n",
    "    s3_client = s3_with_bucket.create_s3_client()\n",
    "    for s3_coordinate in new_files:\n",
    "        bucket = s3_coordinate['bucket']\n",
    "        key = s3_coordinate['key']\n",
    "        data = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "        # first test if the data is a csv file\n",
    "        if data['ContentType'] != 'text/csv':\n",
    "            # continue to the next file\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_csv(data['Body'])\n",
    "        yield AssetMaterialization(asset_key=key, description=\"Created asset from S3 file\")\n",
    "        yield DynamicOut(df, output_name=key)  # Yield a dynamic output for each file\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this seemed to work\n",
    "\n",
    "#TB! load env ariables\n",
    "import our_utilities; our_utilities.load_env_with_substitutions()\n",
    "\n",
    "from dagster import asset, AssetIn, AssetKey, AssetMaterialization, AssetExecutionContext, Output, DynamicOutput, DynamicOut\n",
    "from dagster.core.instance import DagsterInstance\n",
    "\n",
    "# Import DataFrame from pandas\n",
    "from pandas import DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ...resources.resources import MyAWSS3Resource\n",
    "\n",
    "@asset\n",
    "def get_new_s3_files(context: AssetExecutionContext,\n",
    "                     s3_with_bucket: MyAWSS3Resource):\n",
    "    s3_client = s3_with_bucket.create_s3_client()\n",
    "    bucket = s3_with_bucket.bucket_name\n",
    "\n",
    "    dagster_instance = DagsterInstance.get()\n",
    "\n",
    "    paginator = s3_client.get_paginator('list_objects')\n",
    "    new_files = []\n",
    "    for result in paginator.paginate(Bucket=bucket):\n",
    "        for file in result.get('Contents', []):\n",
    "            file_name = file['Key']\n",
    "            if not dagster_instance.has_asset_key(AssetKey(file_name)):\n",
    "                new_files.append({'bucket': bucket, 'key': file_name})\n",
    "    return new_files\n",
    "\n",
    "\n",
    "@asset(out=DynamicOut(dagster_type=DataFrame),\n",
    "       ins={\"new_files\": AssetIn(key=AssetKey(\"get_new_s3_files\"))})\n",
    "def create_assets_from_s3_files(context: AssetExecutionContext, \n",
    "                                s3_with_bucket: MyAWSS3Resource,\n",
    "                                new_files: list):\n",
    "    s3_client = s3_with_bucket.create_s3_client()\n",
    "    for s3_coordinate in new_files:\n",
    "        bucket = s3_coordinate['bucket']\n",
    "        key = s3_coordinate['key']\n",
    "        data = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "        # first test if the data is a csv file\n",
    "        if data['ContentType'] != 'text/csv':\n",
    "            # continue to the next file\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_csv(data['Body'])\n",
    "        yield AssetMaterialization(asset_key=key, description=\"Created asset from S3 file\")\n",
    "        yield DynamicOut(df, output_name=key)  # Yield a dynamic output for each file\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
